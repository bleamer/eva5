### First assignment responses

1. What are Channels and Kernels (according to EVA)?
    
    #### Channels
    Channels can thought of as **minimum** number of abstract, **mutually exclusive** spaces required to model/express a physical quantity / phenomenon. E.g. 
    
    To express the taste of food delicacy, we can split the spaces / channel into sweet, sour, bitter etc. channels. OR. To express a musical song, the song can be visualized as being componsed of vocal channel, guitar channel, percussion channel etc.

    Alternatively, channel is collection of similar features.

    #### Kernels
    Kernels are matrix of number which can be convoluted with other matrix of numbers to achieve desired affect. 

    E.g. Given a 2 image described by matrix of pixel intensities a convolution with [Sobel filter](https://en.wikipedia.org/wiki/Sobel_operator) will help determine all the edge in the image.

1. Why should we (nearly) always use 3x3 kernels?
   
   Few consideration with kernels:
    - Kernels should be small to keep computation costs low, 
    - It is also desirable to have larger kernel size so as to capture the interaction between neighbouring pixels.
    - Kernels should also be odd sized such the result of operatee layer can be placed symmetrically and result layer

    Keeping above considerations in mind Hardware accelerator are optimized of 3x3 kernels, that's why it is optimal to use 3x3 kernels

1. How many times to we need to perform 3x3 convolutions operations to reach close to 1x1 from 199x199 (type each layer output like 199x199 > 197x197...)

    **100** times

    199x199 > 197x197 > 195x195 > 193x193 > 191x191 > 189x189 > 187x187 > 185x185 > 183x183 > 181x181 > 179x179 > 177x177 > 175x175 > 173x173 > 171x171 > 169x169 > 167x167 > 165x165 > 163x163 > 161x161 > 159x159 > 157x157 > 155x155 > 153x153 > 151x151 > 149x149 > 147x147 > 145x145 > 143x143 > 141x141 > 139x139 > 137x137 > 135x135 > 133x133 > 131x131 > 129x129 > 127x127 > 125x125 > 123x123 > 121x121 > 119x119 > 117x117 > 115x115 > 113x113 > 111x111 > 109x109 > 107x107 > 105x105 > 103x103 > 101x101 > 99x99 > 97x97 > 95x95 > 93x93 > 91x91 > 89x89 > 87x87 > 85x85 > 83x83 > 81x81 > 79x79 > 77x77 > 75x75 > 73x73 > 71x71 > 69x69 > 67x67 > 65x65 > 63x63 > 61x61 > 59x59 > 57x57 > 55x55 > 53x53 > 51x51 > 49x49 > 47x47 > 45x45 > 43x43 > 41x41 > 39x39 > 37x37 > 35x35 > 33x33 > 31x31 > 29x29 > 27x27 > 25x25 > 23x23 > 21x21 > 19x19 > 17x17 > 15x15 > 13x13 > 11x11 > 9x9 > 7x7 > 5x5 > 3x3 > 1x1 > 

1. How are kernels initialized?Â 

    Kernels are initialized using random values, more specifically kernels are initialized be using random sampled values from a normal distribution depending on the size of kernel

1. What happens during the training of a DNN?

    A typical inference problem in machine learning can be described as a Function for weights.

    y = F(**X**)

    or

    y = a<sub>1</sub> x<sub>1</sub><sup>p1</sup> + a<sub>2</sub> x<sub>2</sub><sup>p2</sup>+ .. + ..
    a<sub>n</sub> x<sub>n</sub><sup>pn</sup>

    During the training of DNN the intent is determine the most optimal values of coefficients/weights **a**<sub>i</sub><sup>pi</sup>, by looking iterating over known values of **y** and **X** using numerical methods / optimizations.
    Over the iterations the values of weights are readjusted to minimize the loss between acutal **y** value in the value calculated by F(**x**) 